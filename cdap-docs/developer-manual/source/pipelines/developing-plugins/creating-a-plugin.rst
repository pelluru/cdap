.. meta::
    :author: Cask Data, Inc.
    :copyright: Copyright © 2016-2017 Cask Data, Inc.

.. _cdap-pipelines-creating-a-plugin:

=================
Creating a Plugin
=================


.. highlight:: java

Action Plugin
=============
An ``Action`` plugin runs arbitrary logic at the start or end of a batch data pipeline.

In order to implement an Action plugin, you
extend the ``Action`` class. Only one method is required to be implemented::

  run()

.. rubric:: Methods

- ``run()``: Used to implement the functionality of the plugin.
- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.

Example:

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/FilesetMoveAction.java
   :language: java
   :lines: 34-


Post-run Action Plugin
======================
A ``PostAction`` plugin runs arbitrary logic after the end of a pipeline run. 
It can be set to execute based on whether the run completed successfully,
if it failed, or in either case.

The difference between a ``PostAction`` |---| and an ``Action`` that is placed at the
end of a pipeline |---| is that a ``PostAction`` will always be executed even if the pipeline run fails,
while an ``Action`` will only be executed if every stage preceding it successfully runs.

In order to implement an Post-run Action plugin, you extend the ``PostAction`` class.
Only one method is required to be implemented::

  run()

.. rubric:: Methods

- ``run()``: Used to implement the functionality of the plugin.
- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.

Example:

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/FilesetDeletePostAction.java
   :language: java
   :lines: 32-


Batch Source Plugin
===================
A ``BatchSource`` plugin is used as a source of a batch data pipeline. It is used to prepare
and configure the input of a pipeline run.

In order to implement a Batch Source, you extend the
``BatchSource`` class. You need to define the types of the ``KEY`` and ``VALUE`` that the Batch
Source will receive and the type of object that the Batch Source will emit to the
subsequent stage (which could be either a Transformation or a Batch Sink). After defining
the types, only one method is required to be implemented::

  prepareRun()

.. rubric:: Methods

- ``prepareRun()``: Used to configure the input for each run of the pipeline.
  If the ``fieldName`` for a stream or dataset is a macro, their creation will happen during this stage. 
  This is called by the client that will submit the job for the pipeline run.
- ``onRunFinish()``: Used to run any required logic at the end of a pipeline run. This is called
  by the client that submitted the job for the pipeline run.
- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.
- ``initialize()``: Initialize the Batch Source. Guaranteed to be executed before any call
  to the plugin’s ``transform`` method. This is called by each executor of the job. For example,
  if the MapReduce engine is being used, each mapper will call this method.
- ``destroy()``: Destroy any resources created by ``initialize``. Guaranteed to be executed after all calls
  to the plugin’s ``transform`` method have been made. This is called by each executor of the job.
  For example, if the MapReduce engine is being used, each mapper will call this method.
- ``transform()``: This method will be called for every input key-value pair generated by
  the batch job. By default, the value is emitted to the subsequent stage.

Example:

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/TextFileSetSource.java
   :language: java
   :lines: 47-

.. rubric:: Lineage

For plugins that fetch data from non-CDAP sources, the lineage is registered using the ``inputName`` provided
when ``setInput()`` is invoked on ``BatchSourceContext`` in ``prepareRun()``. Note that the ``inputName`` should
be a valid ``DatasetId``. For example::

  @Override
  public void prepareRun(BatchSourceContext context) throws Exception {
    ...
    context.setInput(Input.of("myExternalSource", myInputFormatProvider));
  }

Lineage will be tracked using ``myExternalSource``.

Batch Sink Plugin
=================
A ``BatchSink`` plugin is used to write data in either batch or real-time data pipelines.
It is used to prepare and configure the output of a batch of data from a pipeline run.

In order to implement a Batch Sink, you extend the
``BatchSink`` class. Similar to a Batch Source, you need to define the types of the ``KEY`` and
``VALUE`` that the Batch Sink will write in the Batch job and the type of object that it will
accept from the previous stage (which could be either a Transformation or a Batch Source).

.. highlight:: java

After defining the types, only one method is required to be implemented::

  prepareRun()

.. rubric:: Methods

- ``prepareRun()``: Used to configure the output for each run of the pipeline. This is called by
  the client that will submit the job for the pipeline run.
- ``onRunFinish()``: Used to run any required logic at the end of a pipeline run. This is called
  by the client that submitted the job for the pipeline run.
- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.
- ``initialize()``: Initialize the Batch Sink. Guaranteed to be executed before any call
  to the plugin’s ``transform`` method. This is called by each executor of the job. For example,
  if the MapReduce engine is being used, each mapper will call this method.
- ``destroy()``: Destroy any resources created by ``initialize``. Guaranteed to be executed after all calls
  to the plugin’s ``transform`` method have been made. This is called by each executor of the job.
  For example, if the MapReduce engine is being used, each mapper will call this method.
- ``transform()``: This method will be called for every object that is received from the
  previous stage. The logic inside the method will transform the object to the key-value
  pair expected by the Batch Sink's output format. If you don't override this method, the
  incoming object is set as the key and the value is set to null.

Example:

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/TextFileSetSink.java
   :language: java
   :lines: 46-

.. rubric:: Lineage

For plugins that write data to non-CDAP sinks, the lineage is registered using the ``outputName`` provided
when ``addOutput()`` is invoked on ``BatchSinkContext`` in ``prepareRun()``. Note that the ``outputName`` should
be a valid ``DatasetId``. For example::

  @Override
  public void prepareRun(BatchSinkContext context) throws Exception {
    ...
    context.addOutput(Output.of("myExternalSink", myOutputFormatProvider));
  }

Lineage will be tracked using ``myExternalSink``.

.. highlight:: java

Transformation Plugin
=====================
A ``Transform`` plugin is used to convert one input record into zero or more output records.
It can be used in both batch and real-time data pipelines.

The only method that needs to be implemented is::

  transform()

.. rubric:: Methods

- ``initialize()``: Used to perform any initialization step that might be required during
  the runtime of the ``Transform``. It is guaranteed that this method will be invoked
  before the ``transform`` method.
- ``transform()``: This method contains the logic that will be applied on each
  incoming data object. An emitter can be used to pass the results to the subsequent stage.
- ``destroy()``: Used to perform any cleanup before the plugin shuts down.

Below is an example of a ``StringCase`` that transforms specific fields to lowercase or uppercase.

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/StringCaseTransform.java
   :language: java
   :lines: 36-

If you wanted, you could add to the ``transform`` method a user metric indicating the
number of fields changed. The user metrics can be queried by using the CDAP
:ref:`Metrics HTTP RESTful API <http-restful-api-metrics>`::

  public void transform(StructuredRecord input, Emitter<StructuredRecord> emitter) throws Exception {
    int fieldsChanged = 0;
    . . .
      builder.set(fieldName, record.get(fieldName). . .
      fieldsChanged += 1;
    . . .
    getContext().getMetrics().count("fieldsChanged", fieldsChanged);
  }

.. _cdap-pipelines-creating-a-plugin-error-transformations:

.. highlight:: java

Error Transformation Plugin
===========================
An ``ErrorTransform`` plugin is a special type of ``Transform`` that consumes error records emitted
from the previous stages instead of output records. It is used to transform an ``ErrorRecord`` to zero
or more output records. In addition to the actual error object, an ``ErrorRecord`` exposes the stage the
error was emitted from, an error code, and an error message. Errors can be emitted by ``BatchSource``,
``Transform``, and ``BatchAggregator`` plugins using the ``Emitter`` they receive.
An ``ErrorTransform`` can be used in both batch and real-time data pipelines.

The only method that needs to be implemented is::

  transform()

.. rubric:: Methods

- ``initialize()``: Used to perform any initialization step that might be required during
  the runtime of the ``ErrorTransform``. It is guaranteed that this method will be invoked
  before the ``transform`` method.
- ``transform()``: This method contains the logic that will be applied on each
  incoming ``ErrorRecord`` object. An emitter can be used to pass the results to the subsequent stage.
- ``destroy()``: Used to perform any cleanup before the plugin shuts down.

Below is an example of an ``ErrorCollector`` that adds the error stage, code, and message to each record it receives.

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/ErrorCollector.java
   :language: java
   :lines: 34-

.. _cdap-pipelines-creating-a-plugin-script-transformations:

.. highlight:: java

Script Transformations
----------------------
In the script transformations (*JavaScriptTransform*, *PythonEvaluator*,
and the *ValidatorTransform*), a ``ScriptContext`` object is
passed to the ``transform()`` method::

  function transform(input, context);

The different Transforms that are passed this context object have similar signatures:

.. list-table::
   :widths: 20 80
   :header-rows: 1

   * - Transform
     - Signature
   * - ``JavaScriptTransform``
     - ``{{function transform(input, emitter, context)}}``
   * - ``PythonEvaluator``
     - ``{{function transform(input, emitter, context)}}``
   * - ``ValidatorTransform``
     - ``{{function isValid(input, context)}}``

The ``ScriptContext`` has these methods::

  public Logger getLogger();
  public StageMetrics getMetrics();
  public ScriptLookup getLookup(String table);
  
The context passed by the *ValidatorTransform* has an additional method that returns a validator::

  public Object getValidator(String validatorName);

These methods allow access within the script to CDAP loggers, metrics, lookup tables, and the validator object.

**Logger**

``Logger`` is an `org.slf4j.Logger <http://www.slf4j.org/api/org/slf4j/Logger.html>`__.

For example, a JavaScript transform step can access and write to the *debug* log with::

  context.getLogger().debug('Received record with id ' + input.id);

**StageMetrics**

``StageMetrics`` has these methods:

- ``count(String metricName, int delta)``: Increases the value of the specific metric by
  delta. Metrics name will be prefixed by the stage ID, hence it will be aggregated for
  the current stage.
- ``gauge(String metricName, long value)``: Sets the specific metric to the provided
  value. Metrics name will be prefixed by the stage ID, hence it will be aggregated for
  the current stage.
- ``pipelineCount(String metricName, int delta)``: Increases the value of the specific
  metric by delta. Metrics emitted will be aggregated for the entire pipeline.
- ``pipelineGauge(String metricName, long value)``: Sets the specific metric to the
  provided value. Metrics emitted will be aggregated for the entire pipeline.

**ScriptLookup**

Currently, ``ScriptContext.getLookup(String table)`` only supports :ref:`key-value tables <datasets-index>`.

For example, if a lookup table *purchases* is configured, then you will be able to perform
operations with that lookup table in your script: ``context.getLookup('purchases').lookup('key')``

**Validator Object**

.. highlight:: javascript

For example, in a validator transform, you can retrieve the validator object and call its
functions as part of your JavaScript::

  var coreValidator = context.getValidator("coreValidator");
  if (!coreValidator.isDate(input.date)) {
  . . .


Batch Aggregator Plugin
=======================
A ``BatchAggregator`` plugin is used to compute aggregates over a batch of data.
It is used in both batch and real-time data pipelines.
An aggregation takes place in two steps: *groupBy* and then *aggregate*.

- In the *groupBy* step, the aggregator creates zero or more group keys for each input
  record. Before the *aggregate* step occurs, the CDAP pipeline will take all records that have the
  same group key, and collect them into a group. If a record does not have any of the
  group keys, it is filtered out. If a record has multiple group keys, it will belong to
  multiple groups.

- The *aggregate* step is then called. In this step, the plugin receives group keys and
  all records that had that group key. It is then left to the plugin to decide what to do
  with each of the groups.

In order to implement a Batch Aggregator, you extend the
``BatchAggregator`` class. Unlike a ``Transform``, which operates on a single record at a time, a
``BatchAggregator`` operates on a collection of records. 

.. highlight:: java

.. rubric:: Methods

- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.
- ``initialize()``: Initialize the Batch Aggregator. Guaranteed to be executed before any call
  to the plugin’s ``groupBy`` or ``aggregate`` methods. This is called by each executor of the job.
  For example, if the MapReduce engine is being used, each mapper will call this method.
- ``destroy()``: Destroy any resources created by ``initialize``. Guaranteed to be
  executed after all calls to the plugin’s ``groupBy`` or ``aggregate`` methods have been
  made. This is called by each executor of the job. For example, if the MapReduce engine
  is being used, each mapper will call this method.
- ``groupBy()``: This method will be called for every object that is received from the
  previous stage. This method returns zero or more group keys for each object it recieves.
  Objects with the same group key will be grouped together for the ``aggregate`` method.
- ``aggregate()``: The method is called after every object has been assigned their group keys.
  This method is called once for each group key emitted by the ``groupBy`` method.
  The method recieves a group key as well as an iterator over all objects that had that group key.
  Objects emitted in this method are the output for this stage. 

Example:

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/WordCountAggregator.java
   :language: java
   :lines: 32-


Batch Joiner Plugin
===================
A ``BatchJoiner`` plugin is used to join records over a batch of data.
It can be used in both batch and real-time data pipelines.
A join takes place in two steps: a *joinOn* step followed by a *merge* step.

#. In the *joinOn* step, the joiner creates a join key for each input record.
   the CDAP pipeline will then take all records that have the same join key and collect them into a group.

#. The *merge* step is then called. In this step, the plugin receives a list of all the
   records with same join key based on the type of join (either an *inner* or *outer* join).
   It is then up to the plugin to decide what to emit, in what becomes the final output of the stage.

To implement a Batch Joiner, you extend the ``BatchJoiner`` class. Unlike a ``Transform``,
which operates on a single record at a time, a ``BatchJoiner`` operates on a collection of records.

.. highlight:: java

.. rubric:: Methods

- ``configurePipeline()``: Used to create any streams or datasets, or perform any validation
  on the application configuration that is required by this plugin.
- ``initialize()``: Initialize the Batch Joiner. Guaranteed to be executed before any call
  to the plugin’s ``joinOn`` or ``merge`` methods. This is called by each executor of the job.
  For example, if the MapReduce engine is being used, each mapper will call this method.
- ``prepareRun()``: Prepares a pipeline run. This is run every time before a pipeline runs
  to help set up the run. Here you can set properties such as the number of partitions
  to use when joining and the join key class, if it is not known at compile time.
- ``destroy()``: Destroy any resources created by the ``initialize`` method. Guaranteed to be
  executed after all calls to the plugin’s ``joinOn`` or ``merge`` methods have been
  made. This is called by each executor of the job. For example, if the MapReduce engine
  is being used, each mapper will call this method.
- ``joinOn()``: This method will be called for every object that is received from the
  previous stage. This method returns a join key for each object it recieves.
  Objects with the same join key will be grouped together for the ``merge`` method.
- ``getJoinConfig()``: This method will be called by the CDAP Pipeline to find out the type of join to be performed.
  The config specifies which input stages are ``requiredInputs``.  Records from a required input
  will always be present in the ``merge()`` method.  Records from a non-required input will
  only be present in the ``merge()`` method if they meet the join criteria. In other words,
  if there are no required inputs, a full outer join is performed. If all inputs are
  required inputs, an inner join is performed.
- ``merge()``: This method is called after each object has been assigned a join key.
  The method recieves a join key, an iterator over all objects with that join key, and
  the stage that emitted the object. Objects emitted by this method are the output for this stage.


Spark Compute Plugin
====================
A ``SparkCompute`` plugin is used to transform a collection of input records into a collection
of output records. It can be used in both batch and real-time data pipelines.
It is similar to a ``Transform``, except instead of transforming its input
record by record, it transforms an entire collection. In a ``SparkCompute``
plugin, you are given access to anything you would be able to do in a Spark program. 

In order to implement a Spark Compute Plugin, you extend the ``SparkCompute`` class. 

.. highlight:: java

.. rubric:: Methods

- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.
- ``transform()``: This method is given a Spark RDD (Resilient Distributed Dataset) containing 
  every object that is received from the previous stage. This method then performs Spark operations
  on the input to transform it into an output RDD that will be sent to the next stage.

Example:

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/WordCountCompute.java
   :language: java
   :lines: 35-


Spark Sink Plugin
=================
A ``SparkSink`` plugin is used to perform computations on a collection of input records
and optionally write output data. It can only be used in batch data pipelines.
A ``SparkSink`` is similar to a ``SparkCompute`` plugin except that it has no output.
In a ``SparkSink``, you are given access to anything you would be able to do in a Spark
program. For example, one common use case is to train a machine-learning model in this
plugin.

In order to implement a Spark Sink Plugin, you extend the ``SparkSink`` class. 

.. highlight:: java

.. rubric:: Methods

- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.
- ``run()``: This method is given a Spark RDD (Resilient Distributed Dataset) containing every 
  object that is received from the previous stage. This method then performs Spark operations
  on the input, and usually saves the result to a dataset.

Example:

.. literalinclude:: /../../../cdap-app-templates/cdap-etl/cdap-etl-archetypes/cdap-data-pipeline-plugins-archetype/src/main/resources/archetype-resources/src/main/java/WordCountSink.java
   :language: java
   :lines: 37-


.. highlight:: java

Streaming Source Plugin
=======================
A Streaming Source plugin is used as a source in real-time data pipelines.
It is used to fetch a Spark DStream, which is an object that represents a
collection of Spark RDDs and that produces a new RDD every batch interval of the pipeline. 

In order to implement a Streaming Source Plugin, you extend the ``StreamingSource`` class.

.. rubric:: Methods

- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.
- ``getStream()``: Returns the ``JavaDStream`` that will be used as a source in the pipeline.

Example::

  @Plugin(type = StreamingSource.PLUGIN_TYPE)
  @Name("Twitter")
  @Description("Twitter streaming source.")
  public class TwitterStreamingSource extends StreamingSource<StructuredRecord> {
    private final TwitterStreamingConfig config;

    /**
     * Config class for TwitterStreamingSource.
     */
    public static class TwitterStreamingConfig extends PluginConfig implements Serializable {
      private static final long serialVersionUID = 4218063781909515444L;

      private String consumerKey;

      private String consumerSecret;

      private String accessToken;

      private String accessTokenSecret;

      private String referenceName;
    }

    public TwitterStreamingSource(TwitterStreamingConfig config) {
      this.config = config;
    }

    @Override
    public void configurePipeline(PipelineConfigurer pipelineConfigurer) {
      pipelineConfigurer.getStageConfigurer().setOutputSchema(TwitterConstants.SCHEMA);
    }

    @Override
    public JavaDStream<StructuredRecord> getStream(StreamingContext context) throws Exception {
      JavaStreamingContext javaStreamingContext = context.getSparkStreamingContext();
      // lineage for this source will be tracked with this reference name
      context.registerLineage(config.referenceName);

      // Create authorization from user-provided properties
      ConfigurationBuilder configurationBuilder = new ConfigurationBuilder();
      configurationBuilder.setDebugEnabled(false)
        .setOAuthConsumerKey(config.consumerKey)
        .setOAuthConsumerSecret(config.consumerSecret)
        .setOAuthAccessToken(config.accessToken)
        .setOAuthAccessTokenSecret(config.accessTokenSecret);
      Authorization authorization = new OAuthAuthorization(configurationBuilder.build());

      return TwitterUtils.createStream(javaStreamingContext, authorization).map(
        new Function<Status, StructuredRecord>() {
          public StructuredRecord call(Status status) {
            return convertTweet(status);
          }
        }
      );
    }

    private StructuredRecord convertTweet(Status tweet) {
      // logic to convert a Twitter Status into a CDAP StructuredRecord
    }

  }

.. rubric:: Lineage

The lineage is registered using the ``referenceName`` provided when invoking ``registerLineage()`` on
``StreamingContext`` in ``getStream()``. Note that the ``referenceName`` should be a valid ``DatasetId``.

.. highlight:: java

Windower Plugin
===============
A Windower plugin is used in real-time data pipelines to create sliding windows over the data.
It does this by combining multiple micro batches into larger batches.

A window is defined by its *size* and its *slide interval*. Both are defined in seconds and
must be multiples of the ``batchInterval`` of the pipeline. The *size* defines how much data
is contained in the window. The *slide interval* defines have often a window is created.

For example, consider a pipeline with a ``batchInterval`` of 10 seconds. The pipeline uses a 
``windower`` that has a size of 60 and a slide interval of 30. The input into the ``windower``
will be micro batches containing 10 seconds of data. Every 30 seconds, the windower will
output a batch of data containing the past 60 seconds of data, meaning the previous six 
micro batches that it received as input.

This also means that each window output will overlap (repeat) some of the data from the
previous window. This is useful in calculating aggregates, such as how many "404" responses
did a website send out in the past ten seconds, past minute, past five minutes.

In order to implement a Windower Plugin, you extend the ``Windower`` class.

.. rubric:: Methods

- ``configurePipeline()``: Used to perform any validation on the application configuration
  that is required by this plugin or to create any streams or datasets if the ``fieldName`` for a
  stream or dataset is not a macro.
- ``getWidth()``: Return the width in seconds of windows created by this plugin.
  Must be a multiple of the ``batchInterval`` of the pipeline.
- ``getSlideInterval()``: Get the slide interval in seconds of windows created by this plugin.
  Must be a multiple of the ``batchInterval`` of the pipeline.

Example::

  @Plugin(type = Windower.PLUGIN_TYPE)
  @Name("Window")
  @Description("Creates a sliding window over the data")
  public class Window extends Windower {
    private final Conf conf;

    /**
     * Config for window plugin.
     */
    public static class Conf extends PluginConfig {
      long width;
      long slideInterval;
    }

    public Window(Conf conf) {
      this.conf = conf;
    }

    @Override
    public long getWidth() {
      return conf.width;
    }

    @Override
    public long getSlideInterval() {
      return conf.slideInterval;
    }
  }

.. highlight:: java

Real-Time Source Plugin (Deprecated)
====================================
The only method that needs to be implemented is::

  poll()

.. rubric:: Methods

- ``initialize()``: Initialize the real-time source runtime. Guaranteed to be executed
  before any call to the poll method. Usually used to setup the connection to external
  sources.
- ``configurePipeline()``: Used to create any streams or datasets or perform any validation
  on the application configuration that are required by this plugin.
- ``poll()``: Poll method will be invoked during the run of the plugin and in each call,
  the source is expected to emit zero or more objects for the next stage to process.
- ``destroy()``: Cleanup method executed during the shutdown of the Source. Could be used
  to tear down any external connections made during the initialize method.

Example::

  /**
   * Real-Time Source to poll data from external sources.
   */
  @Plugin(type = "realtimesource")
  @Name("Source")
  @Description("Real-Time Source")
  public class Source extends RealtimeSource<StructuredRecord> {

    private final SourceConfig config;

    public Source(SourceConfig config) {
      this.config = config;
    }

    /**
     * Config class for Source.
     */
    public static class SourceConfig extends PluginConfig {

      @Name("param")
      @Description("Source Param")
      private String param;
      // Note: only primitives (included boxed types) and string are the types that are supported.

    }

    @Nullable
    @Override
    public SourceState poll(Emitter<StructuredRecord> writer, SourceState currentState) {
      // Poll for new data
      // Write structured record to the writer
      // writer.emit(writeDefaultRecords(writer);
      return currentState;
    }

    @Override
    public void initialize(RealtimeContext context) throws Exception {
      super.initialize(context);
      // Get Config param and use to initialize
      // String param = config.param
      // Perform init operations, external operations etc.
    }

    @Override
    public void destroy() {
      super.destroy();
      // Handle destroy lifecycle
    }

    private void writeDefaultRecords(Emitter<StructuredRecord> writer){
      Schema.Field bodyField = Schema.Field.of("body", Schema.of(Schema.Type.STRING));
      StructuredRecord.Builder recordBuilder = StructuredRecord.builder(Schema.recordOf("defaultRecord", bodyField));
      recordBuilder.set("body", "Hello");
      writer.emit(recordBuilder.build());
    }
  }

.. highlight:: java

Source State
------------
**Source State in a Real-Time Source:** Real-time plugins are executed in workers; during
failure, there is the possibility that the data that is emitted from the Source will not
be processed by subsequent stages. In order to avoid such data loss, ``SourceState`` can be
used to persist the information about the external source (for example, an offset) if
supported by the source.

In case of failure, when the poll method is invoked, the offset last persisted is passed
to the poll method, which can be used to fetch the data from the last processed point. The
updated ``SourceState`` information is returned by the poll method. After the data is
processed by any transformations and then finally persisted by the sink, the new
``SourceState`` information is also persisted. This ensures that there will be no data loss in
case of failures.

::

  @Plugin(type = "realtimesource")
  @Name("Demo")
  @Description("Demo Real-Time Source")
  public class DemoSource extends RealtimeSource<String> {
    private static final Logger LOG = LoggerFactory.getLogger(TestSource.class);
    private static final String COUNT = "count";

    @Nullable
    @Override
    public SourceState poll(Emitter<String> writer, SourceState currentState) {
      try {
        TimeUnit.MILLISECONDS.sleep(100);
      } catch (InterruptedException e) {
        LOG.error("Some Error in Source");
      }

      int prevCount;
      if (currentState.getState(COUNT) != null) {
        prevCount = Bytes.toInt(currentState.getState(COUNT));
        prevCount++;
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      } else {
        prevCount = 1;
        currentState = new SourceState();
        currentState.setState(COUNT, Bytes.toBytes(prevCount));
      }

      LOG.info("Emitting data! {}", prevCount);
      writer.emit("Hello World!");
      return currentState;
    }
  }


.. highlight:: java

Real-Time Sink Plugin (Deprecated)
==================================
The only method that needs to be implemented is::

  write()

.. rubric:: Methods

- ``initialize()``: Initialize the real-time sink runtime. Guaranteed to be executed before
  any call to the ``write`` method.
- ``configurePipeline()``: Used to create any datasets or perform any validation
  on the application configuration that are required by this plugin.
- ``write()``: The write method will be invoked for a set of objects that needs to be
  persisted. A ``DataWriter`` object can be used to write data to CDAP streams and/or datasets.
  The method is expected to return the number of objects written; this is used for collecting
  metrics.
- ``destroy()``: Cleanup method executed during the shutdown of the Sink.

Example::

  @Plugin(type = "realtimesink")
  @Name("Demo")
  @Description("Demo Real-Time Sink")
  public class DemoSink extends RealtimeSink<String> {

    @Override
    public int write(Iterable<String> objects, DataWriter dataWriter) {
      int written = 0;
      for (String object : objects) {
        written += 1;
        . . .
      }
      return written;
    }
  }

